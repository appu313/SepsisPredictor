{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7319df59",
   "metadata": {
    "id": "7319df59"
   },
   "source": [
    "# Sepsis Project: (main.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XoysYcVFcs8K",
   "metadata": {
    "id": "XoysYcVFcs8K"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd61b1",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can run this notebook also in your local VS-code ##\n",
    "# =====================================\n",
    "# 1) Setup: Detect Colab and set project folder\n",
    "# =====================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if running in Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "print(\"Running in Colab?\", is_colab)\n",
    "\n",
    "# If in Colab, mount Drive and set path\n",
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    #PROJECT_PATH = '/content/drive/MyDrive/Deep Learning S25 Course Project'\n",
    "    PROJECT_PATH = '/content/drive/MyDrive/Deep Learning S25 Course Project'\n",
    "else:\n",
    "    # Local dev: use current folder or adjust if needed\n",
    "    PROJECT_PATH = os.getcwd()\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Add to sys.path for custom imports\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "# Confirm\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"sys.path includes this folder:\", os.getcwd() in sys.path)\n",
    "\n",
    "# Confirm contents\n",
    "print(\"\\nFolder contents:\")\n",
    "for item in os.listdir():\n",
    "    print(\"-\", item)\n",
    "\n",
    "print(\"\\nData folder contents:\")\n",
    "print(os.listdir(\"data\"))\n",
    "\n",
    "print(\"\\nData_Preparation folder contents:\")\n",
    "print(os.listdir(\"Data_Preparation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 2) Check what's in your data folder\n",
    "# =====================================\n",
    "print(\"Data folder files:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ZktQvEMzuFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing requirements\n",
    "%pip install -q \\\n",
    "    imbalanced-learn \\\n",
    "    imblearn \\\n",
    "    matplotlib \\\n",
    "    numpy \\\n",
    "    pandas \\\n",
    "    scikit-learn \\\n",
    "    seaborn \\\n",
    "    torch \\\n",
    "    tqdm \\\n",
    "    ipywidgets \\\n",
    "    notebook \\\n",
    "    joblib\n",
    "\n",
    "# Checking if cuda is available\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cead64d",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FETRZ8fjNbi3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether data preprocessing step should be computed again\n",
    "# If false, load previously saved preprocessed data\n",
    "RECOMPUTE_DATA_PREPROCESSING = False\n",
    "LOAD_CLEAN_DATA = False\n",
    "\n",
    "# Whether to load raw data for EDA\n",
    "RUN_EDA_AB = False\n",
    "RUN_EDA_A = False\n",
    "RUN_EDA_B = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 3) Data Import and structure\n",
    "# =====================================\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path: use 'data/' now\n",
    "DATA_FILE_AB = 'data/raw/training_set_AB.csv'\n",
    "DATA_FILE_A = 'data/raw/training_set_A.csv'\n",
    "DATA_FILE_B = 'data/raw/training_set_B.csv'\n",
    "\n",
    "\n",
    "# training_set_A: Data from Hospital System A ========>  data_A (Use for training)\n",
    "# training_set_B: Data from Hospital System B ========>  data_B (Use for validation or testing)\n",
    "# training_set_AB: Combined Data from Hospital System A and B ========> (We probably won't use it) - We still can use it in EDA\n",
    "data_AB = None\n",
    "data_A = None\n",
    "data_B = None\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_AB:\n",
    "  data_AB = pd.read_csv(DATA_FILE_AB)\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_A:\n",
    "  data_A = pd.read_csv(DATA_FILE_A)\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_B:\n",
    "  data_B = pd.read_csv(DATA_FILE_B)\n",
    "\n",
    "# Drop columns with 60%+ missingness\n",
    "row_ct = None if data_AB is None else data_AB.shape[0]\n",
    "threshold = None if data_AB is None else int(row_ct * 0.2)\n",
    "data_AB_cleaned = None if data_AB is None else data_AB.dropna(axis=1, thresh=threshold)\n",
    "if not data_AB_cleaned is None:\n",
    "  print(f'Kept {len(data_AB_cleaned.columns.to_list()) - 3} feature columns.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hj8cW48e7_nz",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped loading raw data!' if data_AB is None else data_AB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ge1aOoawcgFr",
   "metadata": {
    "id": "Ge1aOoawcgFr"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uAjpzBqxdKH5",
   "metadata": {
    "id": "uAjpzBqxdKH5"
   },
   "source": [
    "## Label generation & dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yn5gjs0ldSdR",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Asal) Run Label generation & dataset splitting\n",
    "\n",
    "#!ls -l\n",
    "from Data_Preparation import(\n",
    "  stratified_group_k_fold\n",
    ")\n",
    "\n",
    "splits = None\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING:\n",
    "  splits = stratified_group_k_fold(data_AB_cleaned, k=5)\n",
    "\n",
    "  for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "      train_df = data_AB.iloc[train_idx]\n",
    "      test_df = data_AB.iloc[test_idx]\n",
    "\n",
    "      # Count positive labels\n",
    "      train_pos = train_df['SepsisLabel'].sum()\n",
    "      test_pos = test_df['SepsisLabel'].sum()\n",
    "\n",
    "      # Count total labels\n",
    "      train_total = len(train_df)\n",
    "      test_total = len(test_df)\n",
    "\n",
    "      print(f\"\\nFold {fold+1}\")\n",
    "      print(f\"Train size: {train_total}, Positive cases: {train_pos} ({100 * train_pos / train_total:.2f}%)\")\n",
    "      print(f\"Test   size: {test_total}, Positive cases: {test_pos} ({100 * test_pos / test_total:.2f}%)\")\n",
    "\n",
    "else:\n",
    "  print(\"Skipping data preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SKrp2-1cc57y",
   "metadata": {
    "id": "SKrp2-1cc57y"
   },
   "source": [
    "## Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuC8NNOQdAT6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Mitch) Run Data parsing and Handling missing data codes here\n",
    "from Data_Preparation import parse_and_clean_data\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "CLEANED_DATA_DIR = Path('data/cleaned/')\n",
    "if not CLEANED_DATA_DIR.is_dir():\n",
    "  CLEANED_DATA_DIR.mkdir()\n",
    "\n",
    "#Strategy for handling missing values\n",
    "IMPUTE = 'impute'\n",
    "MASK = 'mask'\n",
    "MASK_IMPUTE = 'mask-impute'\n",
    "\n",
    "MISSING_VAL_STRATEGY = IMPUTE # 'mask' 'impute' 'mask-impute'\n",
    "STRATEGIES_TO_LOAD = [MISSING_VAL_STRATEGY] # add any other strategies of interest to load them into the data_dict\n",
    "\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "data_dict = {}\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING:\n",
    "  print(\"Preprocessing data!\")\n",
    "  for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    for strategy in STRATEGIES_TO_LOAD:\n",
    "      train_df = data_AB_cleaned.iloc[train_idx]\n",
    "      test_df = data_AB_cleaned.iloc[test_idx]\n",
    "      train_df_clean = parse_and_clean_data(df=train_df, missing_values=strategy)\n",
    "      test_df_clean = parse_and_clean_data(df=test_df, missing_values=strategy)\n",
    "      data_dict[fold] = {}\n",
    "      data_dict[fold][TRAIN] = {}\n",
    "      data_dict[fold][TEST] = {}\n",
    "      data_dict[fold][TRAIN][strategy] = train_df_clean\n",
    "      data_dict[fold][TEST][strategy] = test_df_clean\n",
    "\n",
    "      train_fname = \"_\".join((str(fold), TRAIN, strategy))\n",
    "      train_fname = \".\".join((train_fname, \"csv\"))\n",
    "      train_df_clean.to_csv(CLEANED_DATA_DIR.joinpath(train_fname), index=False)\n",
    "\n",
    "      test_fname = \"_\".join((str(fold), TEST, strategy))\n",
    "      test_fname = \".\".join((test_fname, \"csv\"))\n",
    "      test_df_clean.to_csv(CLEANED_DATA_DIR.joinpath(test_fname), index=False)\n",
    "elif LOAD_CLEAN_DATA:\n",
    "  print(\"Loading preprocessed data!\")\n",
    "  fpaths = list(CLEANED_DATA_DIR.glob(\"*.csv\"))\n",
    "  for p in tqdm(fpaths):\n",
    "    fold_str, split_set, strategy = (p.name.split(\".\")[0]).split(\"_\")\n",
    "    if strategy in STRATEGIES_TO_LOAD:\n",
    "      curr_df = pd.read_csv(p)\n",
    "      fold = int(fold_str)\n",
    "      if fold in data_dict.keys():\n",
    "        if split_set in data_dict[fold].keys():\n",
    "          data_dict[fold][split_set].update({strategy : curr_df})\n",
    "        else:\n",
    "          data_dict[fold][split_set] = {}\n",
    "          data_dict[fold][split_set][strategy] = curr_df\n",
    "      else:\n",
    "        data_dict[fold] = {}\n",
    "        data_dict[fold][split_set] = {}\n",
    "        data_dict[fold][split_set][strategy] = curr_df\n",
    "else:\n",
    "  print(\"Skipped clean data loading!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4V9rzBIOMkF",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "\n",
    "%ls -l data/cleaned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dyg9A3z73QtK",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped clean data loading!' if not LOAD_CLEAN_DATA else data_dict[0][TRAIN][MISSING_VAL_STRATEGY].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hzcsiUMWTSHA",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped clean data loading!' if not LOAD_CLEAN_DATA else data_dict[0][TEST][MISSING_VAL_STRATEGY].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sKiTdFIOBksB",
   "metadata": {
    "id": "sKiTdFIOBksB"
   },
   "source": [
    "Create RNN sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jpefkc_l0rKo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rnn_sequences(df, feature_cols, label_col='SepsisLabel', group_col='patient_id', time_col='ICULOS', n=3):\n",
    "    \"\"\"\n",
    "    For each patient, creates overlapping sequences of length `n` to predict the next label.\n",
    "\n",
    "    Returns:\n",
    "        sequences: list of (X_seq, y_next) pairs\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "\n",
    "    for _, group in df.groupby(group_col):\n",
    "        #group = group.sort_values(by='index' if 'index' in group.columns else group.index)\n",
    "        group = group.sort_values(by='index' if 'index' in group.columns else time_col)\n",
    "        X = group[feature_cols].values\n",
    "        y = group[label_col].values\n",
    "\n",
    "        if len(X) <= n:\n",
    "            continue  # skip short sequences\n",
    "\n",
    "        for i in range(len(X) - n):\n",
    "            X_seq = X[i:i+n]        # shape: (n, D)\n",
    "            y_next = y[i+n]         # scalar: label at t+n\n",
    "            sequences.append((X_seq, y_next))\n",
    "\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h7osRrbjdbBi",
   "metadata": {
    "id": "h7osRrbjdbBi"
   },
   "source": [
    "## Feature Normalization and Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE_FOLDS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TnC_F6hvdbUp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Aparna) Run Feature Normalization and Addressing Class Imbalance codes here\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from Data_Preparation import(\n",
    "    train_validate_split,\n",
    "    center,\n",
    "    smote_oversample_to_tensor\n",
    ")\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "#SEQ_LEN = 24\n",
    "ID_COL = 'patient_id'\n",
    "TIME_COL = 'ICULOS'\n",
    "LABEL_COL = 'SepsisLabel'\n",
    "\n",
    "PREPROCESSED_DATA_DIR = Path('data/preprocessed')\n",
    "if not PREPROCESSED_DATA_DIR.is_dir():\n",
    "    PREPROCESSED_DATA_DIR.mkdir()\n",
    "\n",
    "if REGENERATE_FOLDS:\n",
    "\n",
    "    for fold in range(5):\n",
    "        FOLD_DIR = PREPROCESSED_DATA_DIR.joinpath('fold_' + str(fold))\n",
    "        if not FOLD_DIR.is_dir():\n",
    "            FOLD_DIR.mkdir()\n",
    "        \n",
    "        TRAIN_DIR = FOLD_DIR.joinpath('train')\n",
    "        if not TRAIN_DIR.is_dir():\n",
    "            TRAIN_DIR.mkdir()\n",
    "            \n",
    "        TEST_DIR = FOLD_DIR.joinpath('test')\n",
    "        if not TEST_DIR.is_dir():\n",
    "            TEST_DIR.mkdir()\n",
    "        \n",
    "        VAL_DIR = FOLD_DIR.joinpath('validate')\n",
    "        if not VAL_DIR.is_dir():\n",
    "            VAL_DIR.mkdir()\n",
    "        \n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        train_df = data_dict[fold]['train']['impute'].copy()\n",
    "        test_df = data_dict[fold]['test']['impute'].copy()\n",
    "        print(f\"Input: \\nTrain shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "        #feature_cols = train_df.drop(columns=[ID_COL, TIME_COL, LABEL_COL]).columns\n",
    "        col_mask = [ID_COL, TIME_COL, LABEL_COL]\n",
    "        feature_cols = [x for x in train_df.columns.to_list() if not x in col_mask]\n",
    "        \n",
    "        # Apply standard scaler to center the data\n",
    "        train_df[feature_cols] = center(train_df, feature_cols)\n",
    "        test_df[feature_cols] = center(test_df, feature_cols)\n",
    "        \n",
    "        # k fold split for train/validate splitting within train set\n",
    "        inner_splits = train_validate_split(train_df)\n",
    "        for i_fold, (train_idx, val_idx) in enumerate(inner_splits):\n",
    "            train_seqs = generate_rnn_sequences(train_df.iloc[train_idx], feature_cols)\n",
    "            val_seqs = generate_rnn_sequences(train_df.iloc[val_idx], feature_cols)\n",
    "            X_train_tensor, y_train_tensor = smote_oversample_to_tensor(\n",
    "                np.array([x for x, y in train_seqs]), \n",
    "                np.array([y for x, y in train_seqs])\n",
    "            )\n",
    "            X_val_tensor = torch.tensor(np.array([x for x, y in val_seqs]), dtype=torch.float32)\n",
    "            y_val_tensor = torch.tensor(np.array([y for x, y in val_seqs]), dtype=torch.float32)\n",
    "            \n",
    "            del train_seqs, val_seqs\n",
    "            \n",
    "            train_path = TRAIN_DIR.joinpath(f'compressed_train_dataset_{i_fold}.pkl.z')\n",
    "            val_path = VAL_DIR.joinpath(f'compressed_val_dataset_{i_fold}.pkl.z')\n",
    "            \n",
    "            print( f\"Writing data to {train_path}: \")\n",
    "            joblib.dump(TensorDataset(X_train_tensor, y_train_tensor), train_path, compress=3)\n",
    "            \n",
    "            print( f\"Writing data to {val_path}: \")\n",
    "            joblib.dump(TensorDataset(X_val_tensor, y_val_tensor), val_path, compress=3)\n",
    "            \n",
    "            del X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    "\n",
    "        \n",
    "        # RNN Sequences for test set\n",
    "        test_sequences = generate_rnn_sequences(test_df, feature_cols)\n",
    "        X_test_tensor = torch.tensor(np.array([x for x, y in test_sequences]), dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(np.array([y for x, y in test_sequences]), dtype=torch.float32)\n",
    "\n",
    "        fname = 'compressed_test_dataset.pkl.z'\n",
    "        path = TEST_DIR.joinpath(fname)\n",
    "        print( f\"Writing data to {path}: \")\n",
    "        joblib.dump(TensorDataset(X_test_tensor, y_test_tensor), path, compress=3)\n",
    "        del test_sequences, X_test_tensor, y_test_tensor, test_df\n",
    "        \n",
    "        fname_train = 'compressed_train_dataset_full.pkl.z'\n",
    "        train_path = TRAIN_DIR.joinpath(fname_train)\n",
    "        train_seqs = generate_rnn_sequences(train_df, feature_cols)\n",
    "        joblib.dump(\n",
    "            TensorDataset(*smote_oversample_to_tensor(\n",
    "                np.array([x for x, y in train_seqs]),\n",
    "                np.array([y for x, y in train_seqs])\n",
    "            )),\n",
    "            train_path,\n",
    "            compress=3\n",
    "        )\n",
    "        del train_seqs, train_df\n",
    "        print('Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SmuNni6-dbyi",
   "metadata": {
    "id": "SmuNni6-dbyi"
   },
   "source": [
    "## Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LXP63qwZdb7x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "\n",
    "!ls -l\n",
    "############################################\n",
    "###  Test Code Cell Please Don't Change  ###\n",
    "############################################\n",
    "# (Ehsan) Run Exploratory data analysis (EDA) codes here\n",
    "# Lactate is the most relevant criteria then the rest of the plotted variables are most relevant\n",
    "# 1. Serum Lactate\n",
    "# 2. White Blood Cell Count (WBC)\n",
    "# 3. Blood Urea Nitrogen (BUN) / Creatinine\n",
    "# 4. Mean Arterial Pressure (MAP) / Systolic BP (SBP)\n",
    "# 5. Heart Rate (HR) & Respiratory Rate (Resp)\n",
    "\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda\n",
    "# Example:\n",
    "#run_eda(data_A, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_eda(data_B, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_eda(data_A, ['Lactate','WBC'])\n",
    "#run_eda(data_B, ['Lactate','WBC'])\n",
    "############################################\n",
    "######### A more comprehensive EDA #########\n",
    "############################################\n",
    "#run_comprehensive_eda\n",
    "# 1) Missingness\n",
    "# 2) Correlation heatmap (drop rows with any missing in features)\n",
    "# 3) Boxplots for each feature by label\n",
    "# 4) KDE overlays (all features in one grid)\n",
    "# 5) PCA scatter\n",
    "# Example:\n",
    "if RUN_EDA_A:\n",
    "    # 1) Automatically select all feature columns except the ones to drop:\n",
    "    to_drop = ['SepsisLabel', 'patient_id', 'Unit1', 'Unit2', 'HospAdmTime']\n",
    "    all_features = [col for col in data_A.columns if col not in to_drop]\n",
    "\n",
    "    # 2) Quick sanity-check\n",
    "    print(\"Running EDA on:\", all_features)\n",
    "\n",
    "    # 3) Call your comprehensive EDA (here we run all steps 1–5):\n",
    "    from Data_Preparation.eda import run_comprehensive_eda\n",
    "    run_comprehensive_eda(data_A, all_features, steps=[1,2])\n",
    "\n",
    "\n",
    "##### Other examples\n",
    "#run_comprehensive_eda(data_AB, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp','O2Sat','Temp','pH','PTT','Glucose','Chloride','Bilirubin_direct'], steps = [1,2])\n",
    "#run_comprehensive_eda(data_A, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_comprehensive_eda(data_B, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "\n",
    "\n",
    "from Data_Preparation.eda import corr_difference_analysis\n",
    "if RUN_EDA_A:\n",
    "    features = [c for c in data_A.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_A,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AN4EcJkojpce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_A EDA      ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_A:\n",
    "    features = [c for c in data_A.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_A)\n",
    "    run_comprehensive_eda(data_A, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_A)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_A,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5pkMhnFjpol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_B EDA      ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_B:\n",
    "    features = [c for c in data_B.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_B)\n",
    "    run_comprehensive_eda(data_B, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_B)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_B,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DfDe4X07jpvl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_AB EDA     ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_AB:\n",
    "    features = [c for c in data_AB.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_AB)\n",
    "    run_comprehensive_eda(data_AB, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_AB)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_AB,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86929da",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24001d",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c0ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LSTM  ---\n",
      "\n",
      "Epoch  1 | train loss 0.2448 | val loss 0.2196 | val AUROC 0.7012\n",
      "Epoch  2 | train loss 0.2144 | val loss 0.2140 | val AUROC 0.7266\n",
      "Epoch  3 | train loss 0.2095 | val loss 0.2150 | val AUROC 0.7349\n",
      "Epoch  4 | train loss 0.2042 | val loss 0.2126 | val AUROC 0.7390\n",
      "Epoch  5 | train loss 0.1992 | val loss 0.2121 | val AUROC 0.7438\n",
      "Epoch  6 | train loss 0.1952 | val loss 0.2118 | val AUROC 0.7447\n",
      "Epoch  7 | train loss 0.1909 | val loss 0.2138 | val AUROC 0.7351\n",
      "Epoch  8 | train loss 0.1871 | val loss 0.2156 | val AUROC 0.7341\n",
      "Epoch  9 | train loss 0.1820 | val loss 0.2283 | val AUROC 0.7095\n",
      "Epoch 10 | train loss 0.1784 | val loss 0.2212 | val AUROC 0.7155\n",
      "╔══════════════════════════════════════════════╗\n",
      "║          Sepsis LSTM Results               ║\n",
      "╠══════════════════════════════════════════════╣\n",
      "  Best Threshold (max F1) : 0.1312481015920639\n",
      "  F1 Score                : 0.23625730994152047\n",
      "  Precision (at max F1)   : 0.16543816543816545\n",
      "  Recall (at max F1)      : 0.4130879345603272\n",
      "  TN (at max F1)          : 6337\n",
      "  FP (at max F1)          : 1019\n",
      "  FN (at max F1)          : 287\n",
      "  TP (at max F1)          : 202\n",
      "  AUROC                   : 0.7155012226570189\n",
      "  AUPRC                   : 0.15281896305275305\n",
      "  Sensitivity (Recall)    :  0.4131\n",
      "  Specificity             : 0.8615╚══════════════════════════════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "#### APARNA ####\n",
    "import os\n",
    "import json\n",
    "from Model_Definitions.lstm import (\n",
    "    SepsisLSTM,\n",
    "    SepsisTransformerDataset,\n",
    "    train_eval_lstm,\n",
    "    SepsisTransformerResult\n",
    ")\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define arguments (replace with your actual paths)\n",
    "\n",
    "processed_data_dir=\"data/preprocessed/AB\"\n",
    "raw_data=\"data/raw/training_set_AB.csv\"\n",
    "\n",
    "\n",
    "# Load metadata and extract number of features\n",
    "metadata_file = os.path.join(processed_data_dir, 'data.json')\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "num_features = metadata['num_features']\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = SepsisTransformerDataset(os.path.join(processed_data_dir, 'train'), raw_data)\n",
    "test_ds = SepsisTransformerDataset(os.path.join(processed_data_dir, 'test'), raw_data)\n",
    "\n",
    "# Initialize model, hyperparameters, and loss\n",
    "model = SepsisLSTM(input_size=num_features)\n",
    "train_params = {\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "print(f\"\\n--- Training LSTM  ---\\n\")\n",
    "(\n",
    "    epochs, train_loss, val_loss,\n",
    "    fpr, tpr,\n",
    "    prec, rec,\n",
    "    threshold, f1, p, r,\n",
    "    cm, auroc, auprc,\n",
    "    sensitivity, specificity\n",
    ") = train_eval_lstm(model, criterion, train_ds, test_ds, train_params)\n",
    "\n",
    "# Print result\n",
    "result = SepsisTransformerResult(threshold, f1, p, r, cm, auroc, auprc, sensitivity, specificity)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b24f9",
   "metadata": {},
   "source": [
    "# Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18fb85",
   "metadata": {},
   "source": [
    "## GRU-D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714c7ef",
   "metadata": {},
   "source": [
    "Ehsan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10935b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## EHSAN #########\n",
    "\n",
    "# Test GRU-D preprocessing end-to-end without interactive plotting\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a non-interactive backend to avoid kernel crashes\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Add Data_Preparation folder to module search path\n",
    "sys.path.append('./Data_Preparation')\n",
    "from grud_preprocessing import generate_grud_input\n",
    "\n",
    "# Load data\n",
    "raw_path = \"data/raw/training_set_A.csv\"\n",
    "if not os.path.exists(raw_path):\n",
    "    raise FileNotFoundError(f\"File not found: {raw_path}\")\n",
    "df = pd.read_csv(raw_path)\n",
    "\n",
    "# Select a single patient\n",
    "patient_id = df['patient_id'].iloc[0]\n",
    "patient_df = df[df['patient_id'] == patient_id].copy()\n",
    "\n",
    "# Define feature columns (exclude metadata and labels)\n",
    "exclude_cols = [\n",
    "    'Age', 'Gender', 'Unit1', 'Unit2',\n",
    "    'HospAdmTime', 'ICULOS', 'SepsisLabel', 'patient_id'\n",
    "]\n",
    "feature_cols = [col for col in patient_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Generate GRU-D inputs\n",
    "X, M, Delta = generate_grud_input(patient_df, features=feature_cols)\n",
    "\n",
    "# Print shapes and example values\n",
    "print(\"GRU-D input shapes:\")\n",
    "print(f\"  X shape   : {X.shape}\")\n",
    "print(f\"  Mask shape: {M.shape}\")\n",
    "print(f\"  Delta shape: {Delta.shape}\")\n",
    "print(\"\\nSample mask values (first row, first 5 features):\", M[0, :5])\n",
    "print(\"Sample delta values (first row, first 5 features):\", Delta[0, :5])\n",
    "\n",
    "# Save the missingness mask plot to a file\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(M.T, aspect='auto', cmap='gray_r')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.title(f'Missingness Mask for Patient {patient_id}')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.tight_layout()\n",
    "plt.savefig('grud_mask_preview.png')\n",
    "print('Mask plot saved to grud_mask_preview.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b47a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## EHSAN #########\n",
    "\n",
    "# Generate GRU‑D inputs for datasets A, B, and AB and save to separate folders\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "\n",
    "# Use non‑interactive backend\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Add preprocessing module to path\n",
    "sys.path.append('./Data_Preparation')\n",
    "from grud_preprocessing import generate_grud_input\n",
    "\n",
    "# Map dataset identifiers to their CSV paths\n",
    "datasets = {\n",
    "    'A':  'data/raw/training_set_A.csv',\n",
    "    'B':  'data/raw/training_set_B.csv',\n",
    "    'AB': 'data/raw/training_set_AB.csv'\n",
    "}\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    'Age', 'Gender', 'Unit1', 'Unit2',\n",
    "    'HospAdmTime', 'ICULOS', 'SepsisLabel', 'patient_id'\n",
    "]\n",
    "\n",
    "base_output_dir = \"data/preprocessed_grud/\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "for set_name, csv_path in datasets.items():\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"File not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    output_dir = os.path.join(base_output_dir, set_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for patient_id, patient_df in tqdm(df.groupby('patient_id'),\n",
    "                                       desc=f\"Processing set {set_name}\"):\n",
    "        patient_df = patient_df.sort_values('ICULOS').reset_index(drop=True)\n",
    "        X, M, Delta = generate_grud_input(patient_df, features=feature_cols)\n",
    "        out_file = os.path.join(output_dir, f\"patient_{patient_id}.npz\")\n",
    "        np.savez_compressed(out_file, X=X, M=M, Delta=Delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d00006",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## EHSAN #########\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1) Manually load train_grud.py as a standalone module named “train_grud”\n",
    "train_grud_path = os.path.abspath(\"Training_Pipeline/train_grud.py\")\n",
    "spec_tg = importlib.util.spec_from_file_location(\"train_grud\", train_grud_path)\n",
    "train_grud = importlib.util.module_from_spec(spec_tg)\n",
    "sys.modules[\"train_grud\"] = train_grud\n",
    "spec_tg.loader.exec_module(train_grud)\n",
    "\n",
    "# 2) Read grud_pipeline.py, rewrite its import to use our train_grud module\n",
    "pipeline_path = os.path.abspath(\"Training_Pipeline/grud_pipeline.py\")\n",
    "source = open(pipeline_path, \"r\").read()\n",
    "# Replace the package import with our standalone module\n",
    "source = source.replace(\n",
    "    \"from Training_Pipeline.train_grud import SepsisGrudDataset, GRUDModel\",\n",
    "    \"from train_grud import SepsisGrudDataset, GRUDModel\"\n",
    ")\n",
    "\n",
    "# 3) Execute the modified pipeline code in its own module namespace\n",
    "spec_gp = importlib.util.spec_from_loader(\"grud_pipeline\", loader=None)\n",
    "grud_pipeline = importlib.util.module_from_spec(spec_gp)\n",
    "exec(source, grud_pipeline.__dict__)\n",
    "\n",
    "# Now pull out the function\n",
    "run_experiments = grud_pipeline.run_experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## EHSAN #########\n",
    "# clean notebook cell\n",
    "run_experiments(\n",
    "    splits=['A','B','AB'],\n",
    "    results_base='results/GRU_D',\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d422fc4",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698e125",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25768033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set A: 100%|██████████| 16267/16267 [00:47<00:00, 345.71it/s]\n",
      "Processing test set A: 100%|██████████| 4069/4069 [00:11<00:00, 352.04it/s]\n",
      "Processing train set B: 100%|██████████| 16000/16000 [00:43<00:00, 364.09it/s]\n",
      "Processing test set B: 100%|██████████| 4000/4000 [00:11<00:00, 352.63it/s]\n",
      "Processing train set AB: 100%|██████████| 32269/32269 [01:33<00:00, 343.53it/s]\n",
      "Processing test set AB: 100%|██████████| 8067/8067 [00:23<00:00, 349.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#### MITCH\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from Data_Preparation import generate_transformer_input, stratified_group_k_fold, center\n",
    "\n",
    "# Use non‑interactive backend\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Map dataset identifiers to their CSV paths\n",
    "datasets = {\n",
    "    'A':  'data/raw/training_set_A.csv',\n",
    "    'B':  'data/raw/training_set_B.csv',\n",
    "    'AB': 'data/raw/training_set_AB.csv'\n",
    "}\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    'Age', 'Gender', 'Unit1', 'Unit2', 'EtCO2',\n",
    "    'HospAdmTime', 'ICULOS', 'SepsisLabel', 'patient_id'\n",
    "]\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "# concatenated_df = pd.read_csv(datasets['AB'])\n",
    "# percent_missing = concatenated_df.isnull().sum() * 100 / len(concatenated_df)\n",
    "# high_missingness_cols = concatenated_df.columns[percent_missing > 95]\n",
    "# print(f'Columns with lots of missing data: {high_missingness_cols}')\n",
    "# low_missingness_cols = concatenated_df.columns[percent_missing <= 95]\n",
    "# print(f'Keeping the following columns:\\n{low_missingness_cols}')\n",
    "\n",
    "\n",
    "base_output_dir = \"data/preprocessed_transformer/\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "for set_name, csv_path in datasets.items():\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"File not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    #feature_cols = [c for c in df.columns if (c not in exclude_cols) and (c not in high_missingness_cols)]\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    output_dir = os.path.join(base_output_dir, set_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # initialize metadata dict\n",
    "    data_metadata_dict = {}\n",
    "    data_metadata_fname = 'data.json'\n",
    "    \n",
    "    # Split into train and test\n",
    "    splits = stratified_group_k_fold(df, 5)\n",
    "    train_idx, test_idx = splits[0]\n",
    "    train_df = df.iloc[train_idx].copy()\n",
    "    test_df = df.iloc[test_idx].copy()\n",
    "    \n",
    "    # Apply standard scaler for feature normalization\n",
    "    train_df[feature_cols] = center(train_df, feature_cols)\n",
    "    test_df[feature_cols] = center(test_df, feature_cols)\n",
    "    \n",
    "    # save class balance metadata\n",
    "    data_metadata_dict['train_sepsis_prevalence'] = train_df['SepsisLabel'].sum() / len(train_df)\n",
    "    data_metadata_dict['test_sepsis_prevalence'] = test_df['SepsisLabel'].sum() / len(test_df)\n",
    "    \n",
    "    # make directories to save data\n",
    "    train_output_dir = os.path.join(output_dir, 'train')\n",
    "    test_output_dir = os.path.join(output_dir, 'test')\n",
    "    os.makedirs(train_output_dir, exist_ok=True)\n",
    "    os.makedirs(test_output_dir, exist_ok=True)\n",
    "    xmean_train = train_df.mean()\n",
    "    \n",
    "    collected_feature_ct = False\n",
    "    \n",
    "    # generate inputs for train set\n",
    "    for patient_id, patient_df in tqdm(train_df.groupby('patient_id'), desc=f\"Processing train set {set_name}\"):\n",
    "        patient_df = patient_df.sort_values('ICULOS').reset_index(drop=True)\n",
    "        X = generate_transformer_input(patient_df, xmean_train, features=feature_cols)\n",
    "        if X is None:\n",
    "            continue\n",
    "        if not collected_feature_ct:\n",
    "            data_metadata_dict['num_features'] = X.shape[-1]\n",
    "            collected_feature_ct = True\n",
    "        out_file = os.path.join(train_output_dir, f\"patient_{patient_id}.npz\")\n",
    "        np.savez_compressed(out_file, X=X)\n",
    "    \n",
    "    # generate inputs for test set\n",
    "    for patient_id, patient_df in tqdm(test_df.groupby('patient_id'), desc=f\"Processing test set {set_name}\"):\n",
    "        patient_df = patient_df.sort_values('ICULOS').reset_index(drop=True)\n",
    "        X = generate_transformer_input(patient_df, xmean_train, features=feature_cols)\n",
    "        if X is None:\n",
    "            continue\n",
    "        out_file = os.path.join(test_output_dir, f\"patient_{patient_id}.npz\")\n",
    "        np.savez_compressed(out_file, X=X)\n",
    "    \n",
    "    # dump metadata to json file\n",
    "    with open(os.path.join(output_dir, data_metadata_fname), 'w') as f:\n",
    "        json.dump(data_metadata_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7063031",
   "metadata": {},
   "source": [
    "### Transformer Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e395eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Model_Definitions import (\n",
    "    Sepsis_Predictor_Encoder,\n",
    "    Sepsis_Predictor_Encoder_Hyperparameters\n",
    ")\n",
    "\n",
    "from Training_Pipeline import (\n",
    "    SepsisTransformerDataset,\n",
    "    SepsisTransformerResult,\n",
    "    train_eval_transformer,\n",
    "    Train_Hyperparameters\n",
    ")\n",
    "\n",
    "base_data_dir = \"data/preprocessed_transformer/\"\n",
    "raw_data_dir = \"data/raw\"\n",
    "\n",
    "split = 'A'\n",
    "raw_csv_fname = 'training_set_' + split + '.csv'\n",
    "\n",
    "train_dir = os.path.join(base_data_dir, split, 'train')\n",
    "test_dir = os.path.join(base_data_dir, split, 'test')\n",
    "raw_csv_path = os.path.join(raw_data_dir, raw_csv_fname)\n",
    "\n",
    "metadata_file = os.path.join(base_data_dir, 'data.json')\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "num_features = metadata['num_features']\n",
    "\n",
    "train_ds = SepsisTransformerDataset(train_dir, raw_csv_path)\n",
    "test_ds = SepsisTransformerDataset(test_dir, raw_csv_path)\n",
    "\n",
    "hyperparams = Sepsis_Predictor_Encoder_Hyperparameters(\n",
    "    embedding_dim=64,\n",
    "    feedforward_hidden_dim=128,\n",
    "    n_heads=4,\n",
    "    activation='relu',\n",
    "    n_layers=6,\n",
    "    dropout_p=0,\n",
    "    pos_encoding_dropout_p=0\n",
    ")\n",
    "\n",
    "train_params = Train_Hyperparameters(\n",
    "    batch_size=16,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "model = Sepsis_Predictor_Encoder(\n",
    "    input_size=num_features,\n",
    "    output_size=1,\n",
    "    hyperparameters=hyperparams\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f'\\n\\n')\n",
    "print(f'------ Starting Training: Split {split} ------\\n')\n",
    "print(f'{hyperparams}\\n{train_params}\\n')\n",
    "\n",
    "eval_result = train_eval_transformer(model, criterion, train_ds, test_ds, train_params)\n",
    "loss_grid, roc_grid, prc_grid, best_thresh_scores, confusion_matrix, auroc, auprc = eval_result\n",
    "epochs, train_loss, eval_loss = loss_grid\n",
    "threshold, f1, precision, recall = best_thresh_scores\n",
    "\n",
    "res = SepsisTransformerResult(\n",
    "    best_thresh=threshold,\n",
    "    f1=f1,\n",
    "    precision=precision,\n",
    "    recall=recall,\n",
    "    confusion_matrix=confusion_matrix,\n",
    "    auroc=auroc,\n",
    "    auprc=auprc\n",
    ")\n",
    "\n",
    "print(f'\\n{res}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
