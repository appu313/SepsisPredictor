{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7319df59",
   "metadata": {
    "id": "7319df59"
   },
   "source": [
    "# Sepsis Project: (main.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XoysYcVFcs8K",
   "metadata": {
    "id": "XoysYcVFcs8K"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd61b1",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can run this notebook also in your local VS-code ##\n",
    "# =====================================\n",
    "# 1) Setup: Detect Colab and set project folder\n",
    "# =====================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if running in Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "print(\"Running in Colab?\", is_colab)\n",
    "\n",
    "# If in Colab, mount Drive and set path\n",
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    #PROJECT_PATH = '/content/drive/MyDrive/Deep Learning S25 Course Project'\n",
    "    PROJECT_PATH = '/content/drive/MyDrive/Deep Learning S25 Course Project'\n",
    "else:\n",
    "    # Local dev: use current folder or adjust if needed\n",
    "    PROJECT_PATH = os.getcwd()\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Add to sys.path for custom imports\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "# Confirm\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"sys.path includes this folder:\", os.getcwd() in sys.path)\n",
    "\n",
    "# Confirm contents\n",
    "print(\"\\nFolder contents:\")\n",
    "for item in os.listdir():\n",
    "    print(\"-\", item)\n",
    "\n",
    "print(\"\\nData folder contents:\")\n",
    "print(os.listdir(\"data\"))\n",
    "\n",
    "print(\"\\nData_Preparation folder contents:\")\n",
    "print(os.listdir(\"Data_Preparation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 2) Check what's in your data folder\n",
    "# =====================================\n",
    "print(\"Data folder files:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ZktQvEMzuFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing requirements\n",
    "# %pip install -q \\\n",
    "#     imbalanced-learn \\\n",
    "#     imblearn \\\n",
    "#     matplotlib \\\n",
    "#     numpy \\\n",
    "#     pandas \\\n",
    "#     scikit-learn \\\n",
    "#     seaborn \\\n",
    "#     torch \\\n",
    "#     tqdm \\\n",
    "#     ipywidgets \\\n",
    "#     notebook \\\n",
    "#     joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cead64d",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FETRZ8fjNbi3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether data preprocessing step should be computed again\n",
    "# If false, load previously saved preprocessed data\n",
    "RECOMPUTE_DATA_PREPROCESSING = True\n",
    "LOAD_CLEAN_DATA = False\n",
    "\n",
    "# Whether to load raw data for EDA\n",
    "RUN_EDA_AB = False\n",
    "RUN_EDA_A = False\n",
    "RUN_EDA_B = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 3) Data Import and structure\n",
    "# =====================================\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path: use 'data/' now\n",
    "DATA_FILE_AB = 'data/raw/training_set_AB.csv'\n",
    "DATA_FILE_A = 'data/raw/training_set_A.csv'\n",
    "DATA_FILE_B = 'data/raw/training_set_B.csv'\n",
    "\n",
    "\n",
    "# training_set_A: Data from Hospital System A ========>  data_A (Use for training)\n",
    "# training_set_B: Data from Hospital System B ========>  data_B (Use for validation or testing)\n",
    "# training_set_AB: Combined Data from Hospital System A and B ========> (We probably won't use it) - We still can use it in EDA\n",
    "data_AB = None\n",
    "data_A = None\n",
    "data_B = None\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_AB:\n",
    "  data_AB = pd.read_csv(DATA_FILE_AB)\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_A:\n",
    "  data_A = pd.read_csv(DATA_FILE_A)\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING or RUN_EDA_B:\n",
    "  data_B = pd.read_csv(DATA_FILE_B)\n",
    "\n",
    "# Drop columns with 60%+ missingness\n",
    "row_ct = None if data_AB is None else data_AB.shape[0]\n",
    "threshold = None if data_AB is None else int(row_ct * 0.2)\n",
    "data_AB_cleaned = None if data_AB is None else data_AB.dropna(axis=1, thresh=threshold)\n",
    "if not data_AB_cleaned is None:\n",
    "  print(f'Kept {len(data_AB_cleaned.columns.to_list()) - 3} feature columns.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hj8cW48e7_nz",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped loading raw data!' if data_AB is None else data_AB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ge1aOoawcgFr",
   "metadata": {
    "id": "Ge1aOoawcgFr"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uAjpzBqxdKH5",
   "metadata": {
    "id": "uAjpzBqxdKH5"
   },
   "source": [
    "## Label generation & dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yn5gjs0ldSdR",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Asal) Run Label generation & dataset splitting\n",
    "\n",
    "#!ls -l\n",
    "from Data_Preparation import(\n",
    "  stratified_group_k_fold\n",
    ")\n",
    "\n",
    "splits = None\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING:\n",
    "  splits = stratified_group_k_fold(data_AB_cleaned, k=5)\n",
    "\n",
    "  for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "      train_df = data_AB.iloc[train_idx]\n",
    "      test_df = data_AB.iloc[test_idx]\n",
    "\n",
    "      # Count positive labels\n",
    "      train_pos = train_df['SepsisLabel'].sum()\n",
    "      test_pos = test_df['SepsisLabel'].sum()\n",
    "\n",
    "      # Count total labels\n",
    "      train_total = len(train_df)\n",
    "      test_total = len(test_df)\n",
    "\n",
    "      print(f\"\\nFold {fold+1}\")\n",
    "      print(f\"Train size: {train_total}, Positive cases: {train_pos} ({100 * train_pos / train_total:.2f}%)\")\n",
    "      print(f\"Test   size: {test_total}, Positive cases: {test_pos} ({100 * test_pos / test_total:.2f}%)\")\n",
    "\n",
    "else:\n",
    "  print(\"Skipping data preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SKrp2-1cc57y",
   "metadata": {
    "id": "SKrp2-1cc57y"
   },
   "source": [
    "## Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuC8NNOQdAT6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Mitch) Run Data parsing and Handling missing data codes here\n",
    "from Data_Preparation import parse_and_clean_data\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "CLEANED_DATA_DIR = Path('data/cleaned/')\n",
    "if not CLEANED_DATA_DIR.is_dir():\n",
    "  CLEANED_DATA_DIR.mkdir()\n",
    "\n",
    "#Strategy for handling missing values\n",
    "IMPUTE = 'impute'\n",
    "MASK = 'mask'\n",
    "MASK_IMPUTE = 'mask-impute'\n",
    "\n",
    "MISSING_VAL_STRATEGY = IMPUTE # 'mask' 'impute' 'mask-impute'\n",
    "STRATEGIES_TO_LOAD = [MISSING_VAL_STRATEGY] # add any other strategies of interest to load them into the data_dict\n",
    "\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "data_dict = {}\n",
    "\n",
    "if RECOMPUTE_DATA_PREPROCESSING:\n",
    "  print(\"Preprocessing data!\")\n",
    "  for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    for strategy in STRATEGIES_TO_LOAD:\n",
    "      train_df = data_AB_cleaned.iloc[train_idx]\n",
    "      test_df = data_AB_cleaned.iloc[test_idx]\n",
    "      train_df_clean = parse_and_clean_data(df=train_df, missing_values=strategy)\n",
    "      test_df_clean = parse_and_clean_data(df=test_df, missing_values=strategy)\n",
    "      data_dict[fold] = {}\n",
    "      data_dict[fold][TRAIN] = {}\n",
    "      data_dict[fold][TEST] = {}\n",
    "      data_dict[fold][TRAIN][strategy] = train_df_clean\n",
    "      data_dict[fold][TEST][strategy] = test_df_clean\n",
    "\n",
    "      train_fname = \"_\".join((str(fold), TRAIN, strategy))\n",
    "      train_fname = \".\".join((train_fname, \"csv\"))\n",
    "      train_df_clean.to_csv(CLEANED_DATA_DIR.joinpath(train_fname), index=False)\n",
    "\n",
    "      test_fname = \"_\".join((str(fold), TEST, strategy))\n",
    "      test_fname = \".\".join((test_fname, \"csv\"))\n",
    "      test_df_clean.to_csv(CLEANED_DATA_DIR.joinpath(test_fname), index=False)\n",
    "elif LOAD_CLEAN_DATA:\n",
    "  print(\"Loading preprocessed data!\")\n",
    "  fpaths = list(CLEANED_DATA_DIR.glob(\"*.csv\"))\n",
    "  for p in tqdm(fpaths):\n",
    "    fold_str, split_set, strategy = (p.name.split(\".\")[0]).split(\"_\")\n",
    "    if strategy in STRATEGIES_TO_LOAD:\n",
    "      curr_df = pd.read_csv(p)\n",
    "      fold = int(fold_str)\n",
    "      if fold in data_dict.keys():\n",
    "        if split_set in data_dict[fold].keys():\n",
    "          data_dict[fold][split_set].update({strategy : curr_df})\n",
    "        else:\n",
    "          data_dict[fold][split_set] = {}\n",
    "          data_dict[fold][split_set][strategy] = curr_df\n",
    "      else:\n",
    "        data_dict[fold] = {}\n",
    "        data_dict[fold][split_set] = {}\n",
    "        data_dict[fold][split_set][strategy] = curr_df\n",
    "else:\n",
    "  print(\"Skipped clean data loading!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4V9rzBIOMkF",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "\n",
    "%ls -l data/cleaned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dyg9A3z73QtK",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped clean data loading!' if not LOAD_CLEAN_DATA else data_dict[0][TRAIN][MISSING_VAL_STRATEGY].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hzcsiUMWTSHA",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Skipped clean data loading!' if not LOAD_CLEAN_DATA else data_dict[0][TEST][MISSING_VAL_STRATEGY].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sKiTdFIOBksB",
   "metadata": {
    "id": "sKiTdFIOBksB"
   },
   "source": [
    "Create RNN sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jpefkc_l0rKo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rnn_sequences(df, feature_cols, label_col='SepsisLabel', group_col='patient_id', time_col='ICULOS', n=3):\n",
    "    \"\"\"\n",
    "    For each patient, creates overlapping sequences of length `n` to predict the next label.\n",
    "\n",
    "    Returns:\n",
    "        sequences: list of (X_seq, y_next) pairs\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "\n",
    "    for _, group in df.groupby(group_col):\n",
    "        #group = group.sort_values(by='index' if 'index' in group.columns else group.index)\n",
    "        group = group.sort_values(by='index' if 'index' in group.columns else time_col)\n",
    "        X = group[feature_cols].values\n",
    "        y = group[label_col].values\n",
    "\n",
    "        if len(X) <= n:\n",
    "            continue  # skip short sequences\n",
    "\n",
    "        for i in range(len(X) - n):\n",
    "            X_seq = X[i:i+n]        # shape: (n, D)\n",
    "            y_next = y[i+n]         # scalar: label at t+n\n",
    "            sequences.append((X_seq, y_next))\n",
    "\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h7osRrbjdbBi",
   "metadata": {
    "id": "h7osRrbjdbBi"
   },
   "source": [
    "## Feature Normalization and Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE_FOLDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TnC_F6hvdbUp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Aparna) Run Feature Normalization and Addressing Class Imbalance codes here\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from Data_Preparation import(\n",
    "    train_validate_split,\n",
    "    center,\n",
    "    smote_oversample_to_tensor\n",
    ")\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "#SEQ_LEN = 24\n",
    "ID_COL = 'patient_id'\n",
    "TIME_COL = 'ICULOS'\n",
    "LABEL_COL = 'SepsisLabel'\n",
    "\n",
    "PREPROCESSED_DATA_DIR = Path('data/preprocessed')\n",
    "if not PREPROCESSED_DATA_DIR.is_dir():\n",
    "    PREPROCESSED_DATA_DIR.mkdir()\n",
    "\n",
    "if REGENERATE_FOLDS:\n",
    "\n",
    "    for fold in range(5):\n",
    "        FOLD_DIR = PREPROCESSED_DATA_DIR.joinpath('fold_' + str(fold))\n",
    "        if not FOLD_DIR.is_dir():\n",
    "            FOLD_DIR.mkdir()\n",
    "        \n",
    "        TRAIN_DIR = FOLD_DIR.joinpath('train')\n",
    "        if not TRAIN_DIR.is_dir():\n",
    "            TRAIN_DIR.mkdir()\n",
    "            \n",
    "        TEST_DIR = FOLD_DIR.joinpath('test')\n",
    "        if not TEST_DIR.is_dir():\n",
    "            TEST_DIR.mkdir()\n",
    "        \n",
    "        VAL_DIR = FOLD_DIR.joinpath('validate')\n",
    "        if not VAL_DIR.is_dir():\n",
    "            VAL_DIR.mkdir()\n",
    "        \n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        train_df = data_dict[fold]['train']['impute'].copy()\n",
    "        test_df = data_dict[fold]['test']['impute'].copy()\n",
    "        print(f\"Input: \\nTrain shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "        #feature_cols = train_df.drop(columns=[ID_COL, TIME_COL, LABEL_COL]).columns\n",
    "        col_mask = [ID_COL, TIME_COL, LABEL_COL]\n",
    "        feature_cols = [x for x in train_df.columns.to_list() if not x in col_mask]\n",
    "        \n",
    "        # Apply standard scaler to center the data\n",
    "        train_df[feature_cols] = center(train_df, feature_cols)\n",
    "        test_df[feature_cols] = center(test_df, feature_cols)\n",
    "        \n",
    "        # k fold split for train/validate splitting within train set\n",
    "        inner_splits = train_validate_split(train_df)\n",
    "        for i_fold, (train_idx, val_idx) in enumerate(inner_splits):\n",
    "            train_seqs = generate_rnn_sequences(train_df.iloc[train_idx], feature_cols)\n",
    "            val_seqs = generate_rnn_sequences(train_df.iloc[val_idx], feature_cols)\n",
    "            X_train_tensor, y_train_tensor = smote_oversample_to_tensor(\n",
    "                np.array([x for x, y in train_seqs]), \n",
    "                np.array([y for x, y in train_seqs])\n",
    "            )\n",
    "            X_val_tensor = torch.tensor(np.array([x for x, y in val_seqs]), dtype=torch.float32)\n",
    "            y_val_tensor = torch.tensor(np.array([y for x, y in val_seqs]), dtype=torch.float32)\n",
    "            \n",
    "            del train_seqs, val_seqs\n",
    "            \n",
    "            train_path = TRAIN_DIR.joinpath(f'compressed_train_dataset_{i_fold}.pkl.z')\n",
    "            val_path = VAL_DIR.joinpath(f'compressed_val_dataset_{i_fold}.pkl.z')\n",
    "            \n",
    "            print( f\"Writing data to {train_path}: \")\n",
    "            joblib.dump(TensorDataset(X_train_tensor, y_train_tensor), train_path, compress=3)\n",
    "            \n",
    "            print( f\"Writing data to {val_path}: \")\n",
    "            joblib.dump(TensorDataset(X_val_tensor, y_val_tensor), val_path, compress=3)\n",
    "            \n",
    "            del X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    "\n",
    "        \n",
    "        # RNN Sequences for test set\n",
    "        test_sequences = generate_rnn_sequences(test_df, feature_cols)\n",
    "        X_test_tensor = torch.tensor(np.array([x for x, y in test_sequences]), dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(np.array([y for x, y in test_sequences]), dtype=torch.float32)\n",
    "\n",
    "        fname = 'compressed_test_dataset.pkl.z'\n",
    "        path = TEST_DIR.joinpath(fname)\n",
    "        print( f\"Writing data to {path}: \")\n",
    "        joblib.dump(TensorDataset(X_test_tensor, y_test_tensor), path, compress=3)\n",
    "        print( f\"Completed!! \")\n",
    "\n",
    "        del test_sequences, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SmuNni6-dbyi",
   "metadata": {
    "id": "SmuNni6-dbyi"
   },
   "source": [
    "## Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LXP63qwZdb7x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "\n",
    "!ls -l\n",
    "############################################\n",
    "###  Test Code Cell Please Don't Change  ###\n",
    "############################################\n",
    "# (Ehsan) Run Exploratory data analysis (EDA) codes here\n",
    "# Lactate is the most relevant criteria then the rest of the plotted variables are most relevant\n",
    "# 1. Serum Lactate\n",
    "# 2. White Blood Cell Count (WBC)\n",
    "# 3. Blood Urea Nitrogen (BUN) / Creatinine\n",
    "# 4. Mean Arterial Pressure (MAP) / Systolic BP (SBP)\n",
    "# 5. Heart Rate (HR) & Respiratory Rate (Resp)\n",
    "\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda\n",
    "# Example:\n",
    "#run_eda(data_A, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_eda(data_B, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_eda(data_A, ['Lactate','WBC'])\n",
    "#run_eda(data_B, ['Lactate','WBC'])\n",
    "############################################\n",
    "######### A more comprehensive EDA #########\n",
    "############################################\n",
    "#run_comprehensive_eda\n",
    "# 1) Missingness\n",
    "# 2) Correlation heatmap (drop rows with any missing in features)\n",
    "# 3) Boxplots for each feature by label\n",
    "# 4) KDE overlays (all features in one grid)\n",
    "# 5) PCA scatter\n",
    "# Example:\n",
    "if RUN_EDA_A:\n",
    "    # 1) Automatically select all feature columns except the ones to drop:\n",
    "    to_drop = ['SepsisLabel', 'patient_id', 'Unit1', 'Unit2', 'HospAdmTime']\n",
    "    all_features = [col for col in data_A.columns if col not in to_drop]\n",
    "\n",
    "    # 2) Quick sanity-check\n",
    "    print(\"Running EDA on:\", all_features)\n",
    "\n",
    "    # 3) Call your comprehensive EDA (here we run all steps 1–5):\n",
    "    from Data_Preparation.eda import run_comprehensive_eda\n",
    "    run_comprehensive_eda(data_A, all_features, steps=[1,2])\n",
    "\n",
    "\n",
    "##### Other examples\n",
    "#run_comprehensive_eda(data_AB, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp','O2Sat','Temp','pH','PTT','Glucose','Chloride','Bilirubin_direct'], steps = [1,2])\n",
    "#run_comprehensive_eda(data_A, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "#run_comprehensive_eda(data_B, ['HCO3','Lactate','WBC','BUN','MAP','HR','Resp'])\n",
    "\n",
    "\n",
    "from Data_Preparation.eda import corr_difference_analysis\n",
    "if RUN_EDA_A:\n",
    "    features = [c for c in data_A.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_A,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AN4EcJkojpce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_A EDA      ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_A:\n",
    "    features = [c for c in data_A.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_A)\n",
    "    run_comprehensive_eda(data_A, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_A)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_A,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5pkMhnFjpol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_B EDA      ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_B:\n",
    "    features = [c for c in data_B.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_B)\n",
    "    run_comprehensive_eda(data_B, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_B)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_B,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DfDe4X07jpvl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ehsan\n",
    "############################################\n",
    "###      Ready to run Dataset_AB EDA     ###\n",
    "############################################\n",
    "###  Please Don't Change  ###\n",
    "from Data_Preparation import run_eda, run_comprehensive_eda, corr_difference_analysis\n",
    "if RUN_EDA_AB:\n",
    "    features = [c for c in data_AB.columns\n",
    "                if c not in ('SepsisLabel','patient_id','Unit1','Unit2','HospAdmTime')]\n",
    "    #run_comprehensive_eda(data_AB)\n",
    "    run_comprehensive_eda(data_AB, all_features, steps=[1,2])\n",
    "    #corr_difference_analysis(data_AB)\n",
    "    diff_matrix, top_changes = corr_difference_analysis(\n",
    "        data_AB,\n",
    "        features,\n",
    "        min_count=50,   # only include features with ≥50 non‐null in each label\n",
    "        top_k=15,\n",
    "        figsize=(8,6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86929da",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680434f",
   "metadata": {},
   "source": [
    "## Training - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ebc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from Training_Pipeline import train\n",
    "from Model_Definitions import Baseline_GRU\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "INPUT_DIM = 39\n",
    "HIDDEN_DIM = 64\n",
    "LAYERS = 2\n",
    "OUT_DIM = 2\n",
    "EPOCHS = 10\n",
    "SAVE = True\n",
    "BASELINE_GRU_MODEL_DIR = Path('models/gru_baseline')\n",
    "\n",
    "if not BASELINE_GRU_MODEL_DIR.is_dir():\n",
    "    BASELINE_GRU_MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "PREPROCESSED_DATA_DIR = Path('data/preprocessed')\n",
    "\n",
    "baseline_gru_history_dict = {}\n",
    "\n",
    "for fold in range(5):\n",
    "    print( f\"Reading alreay processed data for fold {fold}: \")\n",
    "    fname = f'time_series_data_compressed_{fold}.pkl.z'\n",
    "    preprocessed_fold = joblib.load(PREPROCESSED_DATA_DIR.joinpath(fname))\n",
    "    model = Baseline_GRU(\n",
    "                input_size=INPUT_DIM,\n",
    "                hidden_size=HIDDEN_DIM,\n",
    "                num_layers=LAYERS,\n",
    "                output_size=OUT_DIM\n",
    "            )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model, history = train(\n",
    "        dataset_fold=preprocessed_fold,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        num_epochs=EPOCHS,\n",
    "    )\n",
    "    if SAVE:\n",
    "        fname = 'baseline_gru_' + str(fold) + '.pt'\n",
    "        torch.save(model.state_dict(), BASELINE_GRU_MODEL_DIR.joinpath(fname))\n",
    "    baseline_gru_history_dict[fold] = history\n",
    "    del preprocessed_fold\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56b871",
   "metadata": {},
   "source": [
    "## Training - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from Training_Pipeline import train\n",
    "from Model_Definitions import Baseline_LSTM\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "INPUT_DIM = 39\n",
    "HIDDEN_DIM = 64\n",
    "LAYERS = 2\n",
    "OUT_DIM = 2\n",
    "EPOCHS = 10\n",
    "SAVE = True\n",
    "BASELINE_LSTM_MODEL_DIR = Path('models/lstm_baseline')\n",
    "\n",
    "if not BASELINE_LSTM_MODEL_DIR.is_dir():\n",
    "    BASELINE_LSTM_MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "PREPROCESSED_DATA_DIR = Path('data/preprocessed')\n",
    "\n",
    "baseline_lstm_history_dict = {}\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    print( f\"Reading alreay processed data for fold {fold}: \")\n",
    "    fname = f'time_series_data_compressed_{fold}.pkl.z'\n",
    "    preprocessed_fold = joblib.load(PREPROCESSED_DATA_DIR.joinpath(fname))\n",
    "    model = Baseline_LSTM(\n",
    "                input_size=INPUT_DIM,\n",
    "                hidden_size=HIDDEN_DIM,\n",
    "                num_layers=LAYERS,\n",
    "                output_size=OUT_DIM\n",
    "            )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model, history = train(\n",
    "                        dataset_fold=preprocessed_fold,\n",
    "                        model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=criterion,\n",
    "                        num_epochs=EPOCHS,\n",
    "                    )\n",
    "    if SAVE:\n",
    "        fname = 'baseline_lstm_' + str(fold) + '.pt'\n",
    "        torch.save(model.state_dict(), BASELINE_LSTM_MODEL_DIR.joinpath(fname))\n",
    "    baseline_lstm_history_dict[fold] = history\n",
    "    del preprocessed_fold\n",
    "print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
